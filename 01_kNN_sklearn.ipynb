{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN) Algorithm Implementation with the Iris Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a step-by-step guide to implementing the **k-Nearest Neighbors (k-NN)** algorithm using Python and the popular **Iris dataset**. The k-NN algorithm is a foundational classification technique in machine learning, which classifies data points based on the majority class of their nearest neighbors.\n",
    "## Objectives\n",
    "\n",
    "- **Load and Explore**: Understand and prepare the Iris dataset, a classic dataset in machine learning.\n",
    "- **Feature and Target Separation**: Organize data for training by separating features and target labels.\n",
    "- **Data Splitting**: Create training and testing sets to evaluate model performance.\n",
    "- **k-NN Implementation**: Implement the k-NN classifier, experimenting with different values of `K`.\n",
    "- **Model Evaluation**: Measure the accuracy of the k-NN model to assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Iris Dataset\n",
    "\n",
    "We’ll use the **Iris dataset**, a classic dataset in machine learning commonly used for classification tasks. This dataset includes three species of Iris flowers (setosa, versicolor, and virginica), each with four features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "The code below loads the Iris dataset using `datasets.load_iris()` from `sklearn`, then creates a **DataFrame** using `pandas`. This DataFrame combines the features and target labels into a structured format for easy exploration.\n",
    "\n",
    "- `np.c_` is used to concatenate arrays along the columns (second axis). Here, it combines the feature data (`iris['data']`) with the target labels (`iris['target']`), arranging them side by side to form a single array.\n",
    "- This combined array is then passed to `pd.DataFrame`, which organizes it into a tabular format with appropriate column names (`iris['feature_names'] + ['target']`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                      columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Features and Target Variable\n",
    "\n",
    "Separate the **features** and the **target variable** from the dataset.\n",
    "\n",
    "- `x`: This variable contains all the feature columns (sepal length, sepal width, petal length, and petal width), which will be used as input for the k-NN algorithm.\n",
    "- `y`: This variable holds the target column, which indicates the species of each flower.\n",
    "\n",
    "The code below uses `iloc` to select:\n",
    "- `x`: All rows and all columns except the last one.\n",
    "- `y`: All rows and only the last column (target).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= iris_df.iloc[:, :-1]\n",
    "y= iris_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "To evaluate the k-NN algorithm effectively, split the dataset into **training** and **testing** sets. This allows us to train the model on one portion of the data and test its performance on another, ensuring a more reliable evaluation.\n",
    "\n",
    "The `train_test_split` function from `sklearn.model_selection` is used here, with the following parameters:\n",
    "- `test_size=0.2`: Reserves 20% of the data for testing and 80% for training.\n",
    "- `shuffle=True`: Randomly shuffles the data before splitting to ensure randomness.\n",
    "- `random_state=0`: Sets a random seed for reproducibility, so the split remains consistent each time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size = 0.2, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the k-NN Classifier\n",
    "\n",
    "Now that we’ve prepared our training and testing data, we can implement the **k-Nearest Neighbors (k-NN) classifier**. The `KNeighborsClassifier` from `sklearn.neighbors` is used here, where we specify the number of neighbors, `K`, to use for classification.\n",
    "\n",
    "- **Choosing K**: The `K` value represents the number of neighbors considered when classifying a new data point. Different values of `K` can affect the classifier's performance, so it’s a good practice to test multiple values to find the optimal one.\n",
    "\n",
    "1. **Define `K`**: Here, `K=3` is selected initially. You can experiment with other values to see their impact.\n",
    "2. **Initialize Classifier**: `KNeighborsClassifier(K)` initializes the k-NN classifier with `K` neighbors.\n",
    "3. **Train the Model**: The `.fit()` method trains the model using the training features (`x_train`) and labels (`y_train`).\n",
    "4. **Predict**: The `.predict()` method generates predictions for the test set (`x_test`), which we store in `y_pred_sklearn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=3\n",
    "knn=KNeighborsClassifier(K)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred_sklearn= knn.predict(x_test)\n",
    "print(y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model's Accuracy\n",
    "\n",
    "To assess the performance of k-NN classifier, calculate the **accuracy score**, which represents the proportion of correct predictions out of the total predictions made.\n",
    "\n",
    "The `accuracy_score` function from `sklearn.metrics` compares the predicted labels (`y_pred_sklearn`) with the actual labels (`y_test`) from the test set. An accuracy score closer to 1 indicates a higher accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
